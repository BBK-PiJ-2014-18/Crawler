### Crawler for CW5...

##### Naming:

As spec gives naming of some of the classes (e.g. HTMLread, WebCrawler) interfaces are named as HTMLreadInterface or WebCrawlerInterface.

#####Crawl coverage:

The crawler indexes only URLs using the http, https or file protocols. Links including HTML Global Attributes are not indexed.

#####Extent of crawl:

Crawling continues until maxLinks or maxDepth are reached.

(a) maxLinks is reached after that number of pages have been scrapped of URLs.
(b) maxDepth is reached once all links found on pages up to that depth have been scrapped for further URLs.

Default value of maxLinks is 20. Default value of maxDepth is 15.  A programmer can construct an instance of WebCrawler with setting their own values of maxLinks & maxDepth, or can use the default values.

#####Search & our Future Programmer:

The search method is implemented as a default method in the interface WebCrawlerInterface. A future programmer would implement that interface with their own class, extending the work already in place in WebCrawler.  An example file is provided (FutureProgrammersWebCrawler.java) to illustrate this set-up.

The default method in just returns true.

#####Output files:

Three files are generated by each crawl.

The first two have default file names:
(a) crawlattributes.txt (a record of the crawl & a log of any non-valid URLs encountered or links that were not found)
(b) crawltemp.txt (the table of URLs that have been scrapped and their depth, as requested in the spec).

The third file is the results table (the name of this file is passed as a parameter to the crawl method).   If search() returns true then the link is stored in this results table.  As the current default implementation of search() always returns true, this file is currently a record of all pages that have been scrapped for URLs. 

#####A note about file structure:

<li>Class files go in a directory called Crawler.</li>

<li>Output will be saved in a directory called Data, within Crawler.</li>

<li>For testing to work the 11 html files need to be in a directory TestHtml inside a directory (also, sorry, called) Crawler in the directory Crawler. </li>

So the base directory Crawler contains:

(a) The class files

(b) A directory: Data (where output files are found)

(c) A directory: Crawler, which contains a directory TestHtml (which contains the 11 test html files).










